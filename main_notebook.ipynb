{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "#!pip install sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import pickle\n",
    "\n",
    "home_dir = '~/Documents/thesis/datasets/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_subsample(x, y, subsample_size=1.0):\n",
    "\n",
    "    class_xs = []\n",
    "    min_elems = None\n",
    "\n",
    "    for yi in np.unique(y):\n",
    "        elems = x[(y == yi)]\n",
    "        class_xs.append((yi, elems))\n",
    "        if min_elems == None or elems.shape[0] < min_elems:\n",
    "            min_elems = elems.shape[0]\n",
    "\n",
    "    use_elems = min_elems\n",
    "    if subsample_size < 1:\n",
    "        use_elems = int(min_elems*subsample_size)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for ci,this_xs in class_xs:\n",
    "        if len(this_xs) > use_elems:\n",
    "            this_xs = this_xs.reindex(np.random.permutation(this_xs.index))\n",
    "\n",
    "        x_ = this_xs[:use_elems]\n",
    "        y_ = np.empty(use_elems)\n",
    "        y_.fill(ci)\n",
    "\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "\n",
    "    xs = pd.concat(xs)\n",
    "    ys = pd.Series(data=np.concatenate(ys),name='target')\n",
    "\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x):\n",
    "    return (x - x.min(0)) / x.ptp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns_panda(pandaDataframe, blacklist):\n",
    "    for element in blacklist:\n",
    "        del pandaDataframe[element]\n",
    "    return pandaDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data = data.replace(\"na\", 0)\n",
    "    data = data.replace(\"neg\", 0)\n",
    "    data = data.replace(\"pos\", 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_data(with_undersampling=False):\n",
    "    \n",
    "    X_train = pd.read_csv(home_dir + 'scania/training_set_new.csv')\n",
    "    X_test = pd.read_csv(home_dir + 'scania/test_set_new.csv') \n",
    "    \n",
    "    X2_train = pd.read_csv(home_dir + 'scania/live_training_set.csv')  \n",
    "\n",
    "    \n",
    "    X_train = preprocess_data(X_train)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    X2_train = preprocess_data(X2_train)\n",
    "    \n",
    "    y_train = X_train.iloc[:, 0]  # First column in labels\n",
    "    y_test = X_test.iloc[:, 0]  # First column in labels\n",
    "    y2_test = X2_train.iloc[:, 0]  # First column in labels\n",
    "\n",
    "    X_train = X_train.drop(X_train.columns[0], axis=1)  # delete first column of xtrain\n",
    "    X_test = X_test.drop(X_test.columns[0], axis=1)  # delete first column of xtest\n",
    "    X2_train = X2_train.drop(X2_train.columns[0], axis=1)  # delete first column of xtest\n",
    "    \n",
    "    if with_undersampling:\n",
    "        X_train, y_train = balanced_subsample(X_train, y_train)\n",
    "        \n",
    "    return X_train, X_test, X2_train, y_train, y_test, y2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_file(filename, data):\n",
    "    f = open(filename, 'w')\n",
    "    f.write('id,label\\n')\n",
    "    i = 1\n",
    "    for item in data:\n",
    "        f.write('%s' % i)\n",
    "        f.write(',')\n",
    "        f.write('%s' % int(item))\n",
    "        f.write('\\n')\n",
    "        i = i+1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cost(predictions, X_train, X_test, y_train, y_test):\n",
    "    cost_1_instances = 0\n",
    "    cost_2_instances = 0\n",
    "    normal_correct = 0\n",
    "    anomaly_correct = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if predictions[i] == 0 and y_test[i] == 1:  # He predicted normal but it was anomaly\n",
    "            cost_2_instances += 1\n",
    "        elif predictions[i] == 1 and y_test[i] == 0:  # He predicted anomaly but it was normal\n",
    "            cost_1_instances += 1\n",
    "        elif predictions[i] == 0 and y_test[i] == 0: # He predicted normal and it was normal\n",
    "            normal_correct += 1\n",
    "        elif predictions[i] == 1 and y_test[i] == 1: # He predicted anomaly and it was anomaly\n",
    "            anomaly_correct += 1\n",
    "\n",
    "    print ((\"cost_1_instances are {}\".format(cost_1_instances)))\n",
    "    print ((\"cost_2_instances are {}\".format(cost_2_instances)))\n",
    "    print ((\"Correct normal predictions are {}\".format(normal_correct)))\n",
    "    print ((\"Correct anomaly predictions are {}\".format(anomaly_correct)))\n",
    "    print ((\"Total Cost is {}\".format(10*cost_1_instances + 500*cost_2_instances)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X_train,y_train, num_of_features, X_test, y_test):\n",
    "# Build RF classifier to use in feature selection\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "    #create new scoring parameter based on cost function\n",
    "    cost = make_scorer( find_cost, greater_is_better= False)\n",
    "    # Build step forward feature selection\n",
    "    sfs1 = sfs(knn,\n",
    "               k_features= num_of_features,\n",
    "               forward=True,\n",
    "               floating=False,\n",
    "               verbose=2,\n",
    "               scoring=cost,\n",
    "               cv=0)\n",
    "\n",
    "    # Perform SFFS\n",
    "    sfs1 = sfs1.fit(X_train, y_train)\n",
    "    predictions = sfs1.predict(X_test)\n",
    "\n",
    "    feat_cols = list(sfs1.k_feature_idx_)\n",
    "    print(feat_cols)\n",
    "    \n",
    "    return feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_classifier(use_training, clf, X_train, X_test, y_train, y_test, name):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    # predictions = np.round(predictions)\n",
    "\n",
    "    \n",
    "    \n",
    "    if not use_training:\n",
    "        return predictions\n",
    "    else:\n",
    "        prediction_prob = clf.predict_proba(X_test)\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for pr in prediction_prob:\n",
    "            if pr[0] > 0.95:\n",
    "                predictions.append(0)  # Neg\n",
    "            else:\n",
    "                predictions.append(1)  # Pos\n",
    "\n",
    "        accuracy = 0\n",
    "        for i in range(0, len(predictions)):\n",
    "            if predictions[i] == y_test[i]:\n",
    "                accuracy +=1\n",
    "                \n",
    "                \n",
    "        conf_mat = confusion_matrix(y_test, predictions)        \n",
    "        average_precision = average_precision_score(y_test, predictions)\n",
    "        \n",
    "        #find average precision recall\n",
    "        print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "        precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "        #plot precision recall\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "        plt.step(recall, precision, color='b', alpha=0.2,\n",
    "                 where='post')\n",
    "        plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                         color='b')\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "                  average_precision))\n",
    "\n",
    "        write_predictions_to_file(name + '_output.csv.dat', y_test)\n",
    "        find_cost(predictions, X_train, X_test, y_train, y_test)\n",
    "        # print (\"Accuracy of {} is {} %\".format(name, round((accuracy)*100, 5)))\n",
    "\n",
    "        return predictions, accuracy/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data from the csv files\n",
    "X_train, X_test, X2_train, y_train, y_test, y2_test = reset_data(with_undersampling=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-149-4a2576acc875>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-149-4a2576acc875>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    predictions_eval_model, accuracy_eval_model = execute_classifier(True, classifier, x2_train_live, ???,  y2_test, ???, name=name)\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define \"classifiers\" to be used\n",
    "\n",
    "classifiers = {\n",
    "\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(3)\n",
    "}\n",
    "\n",
    "# executing each classifier\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    #feat_cols = feature_selection(X_train,y_train, i,  X_test, y_test)\n",
    "    feat_cols = [8, 37, 38, 70, 75]\n",
    "\n",
    "    x_train_less_feat = X_train.iloc[:, feat_cols]\n",
    "    x_test_less_feat = X_test.iloc[:, feat_cols]\n",
    "    x2_train_live = X2_train.iloc[:, feat_cols]\n",
    "    \n",
    "    # predictions after feature reduction\n",
    "    predictions_tocompare, accuracy_tocompare = execute_classifier(True, classifier, x_train_less_feat, x_test_less_feat, y_train, y_test, name=name)\n",
    "   \n",
    "    predictions_live, accuracy_live = execute_classifier(True, classifier, x_less_feat, x2_train_live, y_train, y2_test, name=name)\n",
    "    \n",
    "    # new model is trained with live data \n",
    "    # missing test data for the new model\n",
    "    predictions_eval_model, accuracy_eval_model = execute_classifier(True, classifier, x2_train_live, ???,  y2_test, ???, name=name)\n",
    "   \n",
    "\n",
    "    print (\"Accuracy of {} is {} %\".format(name, round((accuracy_tocompare)*100, 5)))\n",
    "    print (\"Accuracy of {} live is {} %\".format(name, round((accuracy_live)*100, 5)))\n",
    "    print (\"Accuracy of {} the evaluation model is {} %\".format(name, round((accuracy_eval_model)*100, 5)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 170)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
